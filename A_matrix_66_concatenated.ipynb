{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import numba\n",
    "import cupy as cp\n",
    "#l=np.load('arr.npy')\n",
    "#print(l)\n",
    "# l=np.load('gradient.npy')\n",
    "# print(len(l[0]))\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg6=np.array(np.load(\"err6_in_topK.npy\")) #, allow_pickle=True))\n",
    "agg6=agg6.transpose()\n",
    "A_conc6=np.load(\"A_conc6_in_topK.npy\", allow_pickle=True)                  #np.load(\"A_conc1_in.npy\")\n",
    "A_con=[]\n",
    "iter_no=500\n",
    "for i in range(iter_no):\n",
    "    A_con.append(A_conc6[i].todense())\n",
    "A_conc6=np.array(A_con)\n",
    "A_conc6= np.concatenate(A_conc6, axis=1)\n",
    "#np.save(\"agg3.npy\",agg3)\n",
    "\n",
    "# del err1\n",
    "# del err2\n",
    "# del err3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=A_conc6[:, :1000]\n",
    "# print(A.shape)\n",
    "# print(A)\n",
    "# print(A_conc1[1862324][0:1000])\n",
    "A_conc6=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg6_2=np.array(np.load(\"err6_in_2_topK.npy\")) #, allow_pickle=True))\n",
    "agg6_2=agg6_2.transpose()\n",
    "A_conc6_2=np.load(\"A_conc6_in_2_topK.npy\", allow_pickle=True)                  #np.load(\"A_conc1_in.npy\")\n",
    "A_con=[]\n",
    "iter_no=500\n",
    "for i in range(iter_no):\n",
    "    A_con.append(A_conc6_2[i].todense())\n",
    "A_conc6_2=np.array(A_con)\n",
    "A_conc6_2= np.concatenate(A_conc6_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg6_3=np.array(np.load(\"err6_in_3_topK.npy\")) #, allow_pickle=True))\n",
    "agg6_3=agg6_3.transpose()\n",
    "A_conc6_3=np.load(\"A_conc6_in_3_topK.npy\", allow_pickle=True)                  #np.load(\"A_conc1_in.npy\")\n",
    "A_con=[]\n",
    "iter_no=500\n",
    "for i in range(iter_no):\n",
    "    A_con.append(A_conc6_3[i].todense())\n",
    "A_conc6_3=np.array(A_con)\n",
    "A_conc6_3= np.concatenate(A_conc6_3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg6_4=np.array(np.load(\"err6_in_4_topK.npy\")) #, allow_pickle=True))\n",
    "agg6_4=agg6_4.transpose()\n",
    "A_conc6_4=np.load(\"A_conc6_in_4_topK.npy\", allow_pickle=True)                  #np.load(\"A_conc1_in.npy\")\n",
    "A_con=[]\n",
    "iter_no=500\n",
    "for i in range(iter_no):\n",
    "    A_con.append(A_conc6_4[i].todense())\n",
    "A_conc6_4=np.array(A_con)\n",
    "A_conc6_4= np.concatenate(A_conc6_4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1862327, 1000)\n",
      "(1862327, 5000)\n"
     ]
    }
   ],
   "source": [
    "agg6 = np.concatenate((agg6, agg6_2), axis=1) #, agg6_3, agg6_4, agg6_3 #, agg6_3, agg6_4\n",
    "print(agg6.shape)\n",
    "A_conc6 = np.concatenate((A_conc6, A_conc6_2), axis=1) #, A_conc6_3, A_conc6_4, A_conc6_3 #, A_conc6_3, A_conc6_4\n",
    "print(A_conc6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agg6_2\n",
    "# del agg6_3\n",
    "# del agg6_4\n",
    "del A_conc6_2\n",
    "# del A_conc6_3\n",
    "# del A_conc6_4\n",
    "import torch\n",
    "torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.0\n"
     ]
    }
   ],
   "source": [
    "print( np. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:3\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>, <Managed Device 1>, <Managed Device 2>, <Managed Device 3>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7fae79a5d720 to Device at 0x7fae79a577c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "from numba import jit\n",
    "print(cuda.gpus)\n",
    "numba.cuda.select_device( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 CUDA devices\n",
      "id 0     b'NVIDIA RTX A4000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-bdffda51-dada-8c75-fed9-ef75eda5d187\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 1     b'NVIDIA RTX A4000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 33\n",
      "                                    UUID: GPU-b38425d5-b022-2e33-6867-9135f00e0a68\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 2     b'NVIDIA RTX A4000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 75\n",
      "                                    UUID: GPU-49e06853-74db-a3d6-f078-d3c994043508\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 3     b'NVIDIA RTX A4000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 76\n",
      "                                    UUID: GPU-34ba9026-8ccd-321b-07af-4500d5b79f39\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t4/4 devices are supported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-fdb9d8741d67>:6: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def matrix_comp(A,A_conc,d,iter_no):\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()\n",
    "from numba import jit\n",
    "#@cuda.jit\n",
    "@jit #(target='cuda:1')\n",
    "def matrix_comp(A,A_conc,d,iter_no):\n",
    "    #for t in range(int(d/6)):\n",
    "    for t in range(A_conc.shape[0]):\n",
    "#     if (t%500000)==0:\n",
    "#         print(t)\n",
    "    #for j in range(user_no):\n",
    "        A[t]=np.split(A_conc[t],iter_no)\n",
    "        #A[t]=np.ma.row_stack (A[t])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-fdb9d8741d67>:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"matrix_comp\" failed type inference due to: \u001b[1m\u001b[1mNo implementation of function Function(<built-in function setitem>) found for signature:\n",
      " \n",
      " >>> setitem(array(float64, 3d, C), int64, list(array(uint16, 1d, C))<iv=None>)\n",
      " \n",
      "There are 16 candidate implementations:\n",
      "\u001b[1m  - Of which 16 did not match due to:\n",
      "  Overload of function 'setitem': File: <numerous>: Line N/A.\n",
      "    With argument(s): '(array(float64, 3d, C), int64, list(array(uint16, 1d, C))<iv=None>)':\u001b[0m\n",
      "\u001b[1m   No match.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of setitem at <ipython-input-9-fdb9d8741d67> (12)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 12:\u001b[0m\n",
      "\u001b[1mdef matrix_comp(A,A_conc,d,iter_no):\n",
      "    <source elided>\n",
      "    #for j in range(user_no):\n",
      "\u001b[1m        A[t]=np.split(A_conc[t],iter_no)\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit #(target='cuda:1')\n",
      "<ipython-input-9-fdb9d8741d67>:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"matrix_comp\" failed type inference due to: \u001b[1m\u001b[1mCannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 8:\u001b[0m\n",
      "\u001b[1mdef matrix_comp(A,A_conc,d,iter_no):\n",
      "    <source elided>\n",
      "    #for t in range(int(d/6)):\n",
      "\u001b[1m    for t in range(A_conc.shape[0]):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit #(target='cuda:1')\n",
      "/usr/local/lib/python3.8/dist-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"matrix_comp\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 5:\u001b[0m\n",
      "\u001b[1m#@cuda.jit\n",
      "\u001b[1m@jit #(target='cuda:1')\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/usr/local/lib/python3.8/dist-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 5:\u001b[0m\n",
      "\u001b[1m#@cuda.jit\n",
      "\u001b[1m@jit #(target='cuda:1')\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "<ipython-input-9-fdb9d8741d67>:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"matrix_comp\" failed type inference due to: \u001b[1m\u001b[1mNo implementation of function Function(<built-in function setitem>) found for signature:\n",
      " \n",
      " >>> setitem(array(float64, 3d, C), int64, list(array(uint16, 1d, C))<iv=None>)\n",
      " \n",
      "There are 16 candidate implementations:\n",
      "\u001b[1m    - Of which 16 did not match due to:\n",
      "    Overload of function 'setitem': File: <numerous>: Line N/A.\n",
      "      With argument(s): '(array(float64, 3d, C), int64, list(array(uint16, 1d, C))<iv=None>)':\u001b[0m\n",
      "\u001b[1m     No match.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of setitem at <ipython-input-9-fdb9d8741d67> (12)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 12:\u001b[0m\n",
      "\u001b[1mdef matrix_comp(A,A_conc,d,iter_no):\n",
      "    <source elided>\n",
      "    #for j in range(user_no):\n",
      "\u001b[1m        A[t]=np.split(A_conc[t],iter_no)\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit #(target='cuda:1')\n",
      "/usr/local/lib/python3.8/dist-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"matrix_comp\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 8:\u001b[0m\n",
      "\u001b[1mdef matrix_comp(A,A_conc,d,iter_no):\n",
      "    <source elided>\n",
      "    #for t in range(int(d/6)):\n",
      "\u001b[1m    for t in range(A_conc.shape[0]):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/usr/local/lib/python3.8/dist-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-9-fdb9d8741d67>\", line 8:\u001b[0m\n",
      "\u001b[1mdef matrix_comp(A,A_conc,d,iter_no):\n",
      "    <source elided>\n",
      "    #for t in range(int(d/6)):\n",
      "\u001b[1m    for t in range(A_conc.shape[0]):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "d=11173962\n",
    "iter_no=1000\n",
    "user_no=5\n",
    "A6=np.empty((int(d/6),iter_no,user_no))\n",
    "start=time.time()\n",
    "A6=matrix_comp(A6,A_conc6,d,iter_no)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A_conc6\n",
    "torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import time\n",
    "\n",
    "#print(A)\n",
    "#A=np.array(A)\n",
    "#p,*_ = linalg.lstsq(A[:,:,0], e)\n",
    "d=11173962\n",
    "\n",
    "user_gradient=[]\n",
    "check_gradient=[]\n",
    "ind1=int(d/6)\n",
    "num_users=user_no\n",
    "start=time.time()   \n",
    "for user in range(num_users):\n",
    "#for user in user_no:\n",
    "    user_gradient.append([])\n",
    "    #check_gradient.append(original_gradient[user])\n",
    "for t in range(ind1):\n",
    "    #A_big[t]=np.array(A_big[t])\n",
    "    #e[t]=np.array(e[t])\n",
    "        #p,*_ = linalg.lstsq(A1[t], agg1[t])\n",
    "    p, *_  = linalg.lstsq(A6[t], agg6[t][0:iter_no])\n",
    "    #print(p)\n",
    "    for user in range(num_users):\n",
    "        user_gradient[user].append(p[user])\n",
    "    \n",
    "#check_gradient[user]=original_gradient[user]\n",
    "end=time.time()\n",
    "# ind1=int(d/3)\n",
    "# estimated_grad=least_square(A1,agg1,ind1)\n",
    "# end=time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=np.load(\"original_grad.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o=np.load(\"original_grad.npy\")\n",
    "print(o[2][d-20:d])\n",
    "print(user_gradient[2][int(d/6)-20:int(d/6)])\n",
    "print(user_gradient[2][0:20])\n",
    "print(np.linalg.norm(o[0][int(5*d/6):d][:,0]-user_gradient[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gradient66.npy\",user_gradient)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
